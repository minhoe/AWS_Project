{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A binary to train MNIST using multiple GPUs with synchronous updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CURRENT TF VERSION IS [1.12.0]\n",
      "PACKAGES LOADED\n",
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 12497894620802816270\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 3578738625412227571\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 2495710620587495093\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 15078866944\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "    link {\n",
      "      device_id: 1\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "    link {\n",
      "      device_id: 2\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "    link {\n",
      "      device_id: 3\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "incarnation: 16065825689711561015\n",
      "physical_device_desc: \"device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1b.0, compute capability: 7.0\"\n",
      ", name: \"/device:GPU:1\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 15078866944\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "    link {\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "    link {\n",
      "      device_id: 2\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "    link {\n",
      "      device_id: 3\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "incarnation: 15802441445425271322\n",
      "physical_device_desc: \"device: 1, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1c.0, compute capability: 7.0\"\n",
      ", name: \"/device:GPU:2\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 15078866944\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "    link {\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "    link {\n",
      "      device_id: 1\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "    link {\n",
      "      device_id: 3\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "incarnation: 10435929596260117117\n",
      "physical_device_desc: \"device: 2, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1d.0, compute capability: 7.0\"\n",
      ", name: \"/device:GPU:3\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 15078866944\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "    link {\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "    link {\n",
      "      device_id: 1\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "    link {\n",
      "      device_id: 2\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "incarnation: 15954388568422575746\n",
      "physical_device_desc: \"device: 3, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1e.0, compute capability: 7.0\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import tensorflow.contrib.slim as slim\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "%matplotlib inline  \n",
    "print (\"CURRENT TF VERSION IS [%s]\" % (tf.__version__))\n",
    "print (\"PACKAGES LOADED\")\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(device_lib.list_local_devices())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load MNIST Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape :  (60000, 28, 28)\n",
      "Class dataset :  (60000,)\n",
      "Testing data shape :  (10000, 28, 28)\n",
      "Class dataset :  (10000,)\n"
     ]
    }
   ],
   "source": [
    "mnist = keras.datasets.mnist\n",
    "(trainimg, trainlabel), (testimg, testlabel) = mnist.load_data()\n",
    "\n",
    "# Img size : 28 x 28 x 1\n",
    "# Num classes : 10\n",
    "\n",
    "print('Training data shape : ', trainimg.shape)\n",
    "print('Class dataset : ', trainlabel.shape)\n",
    "print('Testing data shape : ', testimg.shape)\n",
    "print('Class dataset : ', testlabel.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape :  (60000, 784)\n",
      "Class dataset :  (60000, 10)\n",
      "Testing data shape :  (10000, 784)\n",
      "Class dataset :  (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "onehot_mat = np.eye(10)\n",
    "\n",
    "trainimg  = np.reshape(trainimg, [-1, 784])\n",
    "testimg   = np.reshape(testimg, [-1, 784])\n",
    "\n",
    "trainlabel = np.concatenate([[onehot_mat[int(x),:]] for x in trainlabel], axis=0)\n",
    "testlabel   = np.concatenate([[onehot_mat[int(x),:]] for x in testlabel], axis=0)\n",
    "\n",
    "print('Training data shape : ', trainimg.shape)\n",
    "print('Class dataset : ', trainlabel.shape)\n",
    "print('Testing data shape : ', testimg.shape)\n",
    "print('Class dataset : ', testlabel.shape)\n",
    "# train_images_flat = sess.run(tf.reshape(train_images, shape=(N_TRN,-1)))\n",
    "# test_images_flat  = sess.run(tf.reshape(test_images, shape=(N_TST,-1)))\n",
    "# train_labels_1h   = sess.run(tf.one_hot(train_labels, depth=n_classes))\n",
    "# test_labels_1h    = sess.run(tf.one_hot(test_labels, depth=n_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_input   = 784\n",
    "n_classes = 10\n",
    "\n",
    "N_TRN = len(trainlabel)\n",
    "N_TST = len(testlabel)\n",
    "\n",
    "BATCH_SIZE = 0\n",
    "NUM_GPUS   = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network ready\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('var'):\n",
    "    x = tf.placeholder('float', [None, n_input])\n",
    "    y = tf.placeholder('float', [None, n_classes])\n",
    "    is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "def lrelu(x, leak=0.2, name='lrelu'):\n",
    "    with tf.variable_scope(name):\n",
    "        f1 = 0.5 * (1 + leak)\n",
    "        f2 = 0.5 * (1 - leak)\n",
    "        return f1 * x + f2 * abs(x)\n",
    "\n",
    "def CNN(inputs, is_training=True):\n",
    "    x   = tf.reshape(inputs, [-1, 28, 28, 1])\n",
    "    \n",
    "    batch_norm_params = {'is_training': is_training, 'decay': 0.9\n",
    "                        , 'updates_collections': None}\n",
    "    init_func = tf.truncated_normal_initializer(stddev=0.01)\n",
    "    \n",
    "    net = slim.conv2d(x, 32, kernel_size=[5,5], padding='SAME' \n",
    "                     , activation_fn       = lrelu\n",
    "                     , weights_initializer = init_func\n",
    "                     , normalizer_fn       = slim.batch_norm\n",
    "                     , normalizer_params   = batch_norm_params\n",
    "                     , scope='conv1')\n",
    "    net = slim.max_pool2d(net, [2, 2], scope='pool1')\n",
    "    net = slim.conv2d(x, 64, kernel_size=[5,5], padding='SAME' \n",
    "                     , activation_fn       = lrelu\n",
    "                     , weights_initializer = init_func\n",
    "                     , normalizer_fn       = slim.batch_norm\n",
    "                     , normalizer_params   = batch_norm_params\n",
    "                     , scope='conv2')\n",
    "    net = slim.max_pool2d(net, [2, 2], scope='pool2')\n",
    "    net = slim.flatten(net, scope='flatten3')\n",
    "    net = slim.fully_connected(net, 1024\n",
    "                     , activation_fn       = lrelu\n",
    "                     , weights_initializer = init_func\n",
    "                     , normalizer_fn       = slim.batch_norm\n",
    "                     , normalizer_params   = batch_norm_params\n",
    "                     , scope='fc4')\n",
    "    net = slim.dropout(net, keep_prob=0.7, is_training=is_training, scope='dr')\n",
    "    out = slim.fully_connected(net, n_classes\n",
    "                     , activation_fn       = None\n",
    "                     , weights_initializer = init_func\n",
    "                     , normalizer_fn       = None\n",
    "                     , scope='fco')\n",
    "    return out\n",
    "print('Network ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tower_loss(scope, images, labels):\n",
    "    \"\"\"Calculate the total loss on a single tower running the MNIST model.\n",
    "\n",
    "    Args:\n",
    "    scope: unique prefix string identifying the MNIST tower, e.g. 'tower_0'\n",
    "    images: Images. 2D tensor of shape [batch_size, :].\n",
    "    labels: Labels. 1D tensor of shape [batch_size].\n",
    "    Returns:\n",
    "     Tensor of shape [] containing the total loss for a batch of data\n",
    "    \"\"\"\n",
    "    # Prediction\n",
    "    pred = CNN(images, is_training)\n",
    "\n",
    "    # LOSS\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "            labels=labels, logits=pred), name='total_loss')\n",
    "    \n",
    "    return cost\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_gradients(tower_grads):\n",
    "    \"\"\"Calculate the average gradient for each shared variable across all towers.\n",
    "    Note that this function provides a synchronization point across all towers.\n",
    "    \n",
    "    Args:\n",
    "    tower_grads: List of lists of (gradient, variable) tuples. The outer list\n",
    "      is over individual gradients. The inner list is over the gradient\n",
    "      calculation for each tower.\n",
    "    Returns:\n",
    "     List of pairs of (gradient, variable) where the gradient has been averaged\n",
    "     across all towers.\n",
    "    \"\"\"\n",
    "    \n",
    "    average_grads = []\n",
    "    for grad_and_vars in zip(*tower_grads):\n",
    "        # Note that each grad_and_vars looks like the following:\n",
    "        #   ((grad0_gpu0, var0_gpu0), ... , (grad0_gpuN, var0_gpuN))\n",
    "        \n",
    "        grad = []\n",
    "        \n",
    "        for g, _ in grad_and_vars:\n",
    "            # Add 0 dimension to the gradients to represent the tower.\n",
    "            expanded_g = tf.expand_dims(g, 0)\n",
    "\n",
    "            # Append on a 'tower' dimension which we will average over below.\n",
    "            grads.append(expanded_g)\n",
    "            \n",
    "        # Average over the 'tower' dimension\n",
    "        grad = tf.concat(axis=0, values=grads)\n",
    "        grad = tf.reduce_mean(grad, 0)\n",
    "        \n",
    "        # Keep in mind that the Variables are redundant because they are shared\n",
    "        # across towers. So .. we will just return the first tower's pointer to\n",
    "        # the Variable.\n",
    "        \n",
    "        v = grad_and_vars[0][1]\n",
    "        grad_and_var = (grad, v)\n",
    "        average_grads.append(grad_and_var)\n",
    "        \n",
    "    return average_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    \"\"\"Train MNIST for a number of steps.\"\"\"\n",
    "    \n",
    "    with tf.Graph().as_default(), tf.devide('/CPU:0'):\n",
    "        # Create a variable to count the number of train() calls. This equals the\n",
    "        # number of batches processed * FLAGS.num_gpus.\n",
    "        global_step = tf.get_variable(\n",
    "            'global_step', [], \n",
    "            initializer=tf.constant_initializer(0), trainable=False)\n",
    "        \n",
    "        # Calculate the learning rate schedule\n",
    "        num_batches_per_epoch = N_TRN / BATCH_SIZE / NUM_GPUS\n",
    "        \n",
    "        # Create an optimizer that performs gradient descent.\n",
    "        optm = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "        \n",
    "        # Get images and labels for MNIST\n",
    "        batch_queue = tf.contrib.slim.prefetch_queue.prefetch_queue(\n",
    "            [trainimg, trainlabel], capacity=2 * NUM_GPUS)\n",
    "        \n",
    "        # Calculate the gradients for each model tower.\n",
    "        tower_grads = []\n",
    "        with tf.variable_scope(tf.get_variable_scope()):\n",
    "            for i in range(NUM_GPUS):\n",
    "                with tf.device('/GPU:%d' % i):\n",
    "                    with tf.name_scope('%s_%d' % ('TOWER', i)) as scope:\n",
    "                        # Dequeues one batch for the GPU\n",
    "                        image_batch, label_batch = batch_queue.dequeue()\n",
    "                        \n",
    "                        # Calculate the loss for one tower of the CIFAR model. This function\n",
    "                        # constructs the entire MNIST model but shares the variables across\n",
    "                        # all towers.\n",
    "                        loss = tower_loss(scope, image_batch, label_batch)                        \n",
    "                        \n",
    "                        # Resue variables for the next tower\n",
    "                        tf.get_variable_scope().reuse_variables()\n",
    "                        \n",
    "                        # Calculate the gradients for the batch of data on this CIFAR tower.\n",
    "                        grads = optm.compute_gradients(loss)\n",
    "                        \n",
    "                        # Keep track of the gradients across all towers.\n",
    "                        tower_grads.append(grads)\n",
    "        \n",
    "        # with~~\n",
    "        # We must calculate the mean of each gradient. Note that this is the\n",
    "        # synchronization point across all towers.\n",
    "        grads = average_gradients(tower_grads)\n",
    "        \n",
    "        # Add histograms for gradients\n",
    "        \n",
    "        # Apply the gradients to adjust the shared variables.\n",
    "        apply_gradient_op = optm.apply_gradients(grads, global_step=global_step)\n",
    "        \n",
    "        # Track the moving averages of all trainable variables.\n",
    "        variable_averages = tf.train.ExponentialMovingAverage(\n",
    "            0.01, global_step)\n",
    "        variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(argv=None):\n",
    "    print('hi!')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi!\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2971: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    tf.app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
